# -*- coding: utf-8 -*-
"""generate train hist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p4BBO1BBzsCuvkOuqFm3xpeEN-fi544I
"""

# Importing the required libraries

import numpy as np
import cv2
import os
import pickle
import sys
from scipy import ndimage
from scipy.spatial import distance
from sklearn.cluster import KMeans
from sklearn.cluster import MiniBatchKMeans

n_classes=36
clustering_factor=7

import tensorflow as tf

# Detect hardware
try:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
except ValueError:
  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)

#path='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Preprocessing'
#sys.path.append(path)

#from _surf_feature_extraction import surf_features
!pip install opencv-python==3.4.2.16
!pip install opencv-contrib-python==3.4.2.16

def surf_features(images):
  surf_descriptors_class_by_class={}
  surf_descriptors_list=[]
  surf=cv2.xfeatures2d.SURF_create()
  for key,value in images.items():
    print(key, "Started")
    features=[]
    for img in value:
      kp,desc=surf.detectAndCompute(img,None)
      surf_descriptors_list.extend(desc)
      features.append(desc)
    surf_descriptors_class_by_class[key]=features
    print(key," Completed!")
  return [surf_descriptors_list,surf_descriptors_class_by_class]

# Implementing Bag of Features Model

# Creating a visual dictionary using only the train dataset 
# K-means clustering alogo takes only 2 parameters which are number of clusters (k) and descrpitors list
# It reurn an array which holds central points

def minibatchkmeans(k, descriptors_list):
  kmeans=MiniBatchKMeans(n_clusters=k)
  print("MiniBatchKMeans Initialized!")
  kmeans.fit(descriptors_list)
  print("Clusters Created!")
  visual_words=kmeans.cluster_centers_
  return visual_words, kmeans

# Loading train images into dictionaries which holds all images category by category

def load_images_by_category(folder):
  images={}
  for label in os.listdir(folder):
    print(label," started")
    category=[]
    path=folder+'/'+label
    for image in os.listdir(path):
      img=cv2.imread(path+'/'+image)
      #new_img=cv2.resize(img,(128,128))
      if img is not None:
        category.append(img)
    images[label]=category
    print(label, "ended")
  return images

'''# Finding the centre to which a feature belong

def find_index(image,center):
  count=0
  ind=0
  for i in range(len(center)):
    if(i==0):
      count=distance.euclidean(image,center[i])
    else:
      dist=distance.euclidean(image,center[i])
      if(dist<count):
        ind=i
        count=dist
  return ind'''

'''# Creating histograms for train images

# Function takes 2 parameters. The first one is a dictionary that holds the descriptors that are separated class by class 
# And the second parameter is an array that holds the central points (visual words) of the k means clustering
# Returns a dictionary that holds the histograms for each images that are separated class by class. 

def image_class(all_bows,centers):
  features_dict={}
  for key,value in all_bows.items():
    print(key," Started!")
    category=[]
    for img in value:
      histogram=np.zeros(len(centers))
      for each_feature in img:
        ind=find_index(each_feature,centers)
        histogram[ind]+=1
      category.append(histogram)
    features_dict[key]=category
    print(key," Completed!")
  return features_dict'''

# Creating histograms for train images

# Function takes 2 parameters. The first one is a dictionary that holds the descriptors that are separated class by class 
# And the second parameter is the clustered model
# Returns a dictionary that holds the histograms for each images that are separated class by class. 

def create_histogram(all_descs,kmeans):
  features_dict={}
  for key,value in all_descs.items():
    print(key," Started!")
    category=[]
    for desc in value:
      raw_words=kmeans.predict(desc)
      hist = np.array(np.bincount(raw_words,minlength=n_classes*clustering_factor))
      category.append(hist)
    features_dict[key]=category
    print(key," Completed!")
  return features_dict

train_folder='/content/drive/My Drive/Colab Notebooks/ISL Recognition/ISL Datasets/Train-Test/Train'

train_images=load_images_by_category(train_folder)
print(len(train_images))

print(len(train_images['a'][0][0]))

file_name='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/train_images'
outfile=open(file_name,'wb')
pickle.dump(train_images,outfile)
outfile.close()

filename='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/train_images'

infile = open(filename,'rb')
train_images = pickle.load(infile)
infile.close()

(len(train_images['a'][1]))

#Extracting surf features from each image stored in train_images list

surfs=surf_features(train_images)
all_train_descriptors=surfs[0]
train_descriptors_by_class=surfs[1]

print(len(surfs[0]))
print(len(surfs[1]['0'][1]))

file_name='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/descriptors_list'
outfile=open(file_name,'wb')
pickle.dump(descriptors_list,outfile)
outfile.close()

file_name='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/all_bow_features'
outfile=open(file_name,'wb')
pickle.dump(all_bow_features,outfile)
outfile.close()

filename='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/descriptors_list'

infile = open(filename,'rb')
descriptors_list = pickle.load(infile)
infile.close()

filename='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/all_bow_features'

infile = open(filename,'rb')
all_bow_features = pickle.load(infile)
infile.close()

# Calling MiniBatchkmeans function and getting central points
visual_words,kmeans=minibatchkmeans(150,all_train_descriptors)

file_name='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/kmeans150'
outfile=open(file_name,'wb')
pickle.dump(kmeans,outfile)
outfile.close()

file_name='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/visual_words'
outfile=open(file_name,'wb')
pickle.dump(visual_words,outfile)
outfile.close()

filename='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/visual_words'

infile = open(filename,'rb')
visual_words = pickle.load(infile)
infile.close()

'''# Calling image_class function and getting histogram for each image
bows_train=image_class(all_bow_features,visual_words)'''

# Calling image_class function and getting histogram for each image
bows_train=create_histogram(train_descriptors_by_class,kmeans)

print((bows_train['a'][0][1]))

file_name='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/bows_train'
outfile=open(file_name,'wb')
pickle.dump(bows_train,outfile)
outfile.close()

filename='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/bows_train'

infile = open(filename,'rb')
bows_train = pickle.load(infile)
infile.close()

print(len(bows_train))

print(bows_train['z'][0][13])

print(len(bows_train['2'][1]))

import csv
loc='/content/drive/My Drive/Colab Notebooks/ISL Recognition/Saved Files/Train150.csv'
with open(loc,'w',newline='') as file:
  writer=csv.writer(file)
  header=[]
  for i in range (1,151):
    header.append(str('pixel')+str(i))
  header.append('Label')
  writer.writerow(header)
  count=0
  for label in bows_train:
     # print(len(bows_train[label]))
    for i in range(len(bows_train[label])):
      list=[]
      for j in range(150):
        list.append(bows_train[label][i][j])
      list.append(label)
      writer.writerow(list)

